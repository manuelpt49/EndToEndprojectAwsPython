{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries needed\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings to connect with AWS services\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_REGION = \"\"\n",
    "SCHEMA_NAME = \"\"\n",
    "S3_STAGING_DIR = \"\"\n",
    "S3_BUCKET_NAME = \"\"\n",
    "S3_OUTPUT_DIRECTORY = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to Athena to query data\n",
    "athena_client = boto3.client(\n",
    "    \"athena\",\n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {}\n",
    "def download_and_load_query_results(\n",
    "        client: boto3.client, query_response: Dict) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to download files from S3 to the local machine\n",
    "    '''\n",
    "    while True:\n",
    "        try:\n",
    "            client.get_query_results(\n",
    "                QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as err:\n",
    "            if \"not yet finished\" in str(err):\n",
    "                time.sleep(0.001)\n",
    "            else:\n",
    "                raise err\n",
    "    \n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id = AWS_ACCESS_KEY,\n",
    "        aws_secret_access_key = AWS_SECRET_KEY,\n",
    "        region_name = AWS_REGION\n",
    "    )\n",
    "    s3_client.download_file(\n",
    "        S3_BUCKET_NAME,\n",
    "        f\"{S3_OUTPUT_DIRECTORY}/{query_response['QueryExecutionId']}.csv\",\n",
    "        temp_file_location,\n",
    "    )\n",
    "    return pd.read_csv(temp_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data inside Notebook to make some transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making queries to dataset of each table through Athena and downloading usign function declared above\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.enigma_jhud LIMIT 7880;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "enigma_jhud = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.countrycode;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "countrycode = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.countypopulation;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "countypopulation = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.rearc_usa_hospital_beds;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "rearc_usa_hospital_beds = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.state_abv;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "state_abv = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.states_daily;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "states_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.us_county;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "us_county = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.us_daily;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "us_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.us_states;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "us_states = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString = \"SELECT * FROM covid_dataset.us_total_latest;\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    }\n",
    ")\n",
    "us_total_latest = download_and_load_query_results(athena_client, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing state_abv DF putting first row as a column\n",
    "cols = state_abv.iloc[0]\n",
    "new_df = state_abv[1:]\n",
    "new_df.columns = cols\n",
    "state_abv = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dim and Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factCovid1 = enigma_jhud[['fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active']]\n",
    "factCovid2 = states_daily[['fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged']]\n",
    "factCovid = pd.merge(factCovid1, factCovid2, on='fips', how='inner')\n",
    "factCovid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimRegion1 = enigma_jhud[['fips','province_state', 'country_region', 'latitude', 'longitude']]\n",
    "dimRegion2 = us_county[['fips', 'county', 'state']]\n",
    "dimRegion = pd.merge(dimRegion1, dimRegion2, \"inner\", on='fips')\n",
    "dimRegion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = states_daily[['fips','date']]\n",
    "dimDate['date'] = pd.to_datetime(dimDate['date'], format='%Y%m%d')\n",
    "dimDate['year'] = dimDate['date'].dt.year\n",
    "dimDate['month'] = dimDate['date'].dt.month\n",
    "dimDate['day'] = dimDate['date'].dt.day\n",
    "dimDate['day_of_week'] = dimDate['date'].dt.dayofweek\n",
    "dimDate\n",
    "\n",
    "dimHospital = rearc_usa_hospital_beds[['fips','state_name','latitude', 'longtitude', 'hq_address', 'hospital_name', 'hospital_type', 'hq_city', 'hq_state']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dim and Fact tables into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = S3_BUCKET_NAME\n",
    "#To save our data into S3 as binary format\n",
    "csv_buffer = StringIO()\n",
    "#Save our table into the buffer\n",
    "factCovid.to_csv(csv_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3',\n",
    "        aws_access_key_id = AWS_ACCESS_KEY,\n",
    "        aws_secret_access_key = AWS_SECRET_KEY,\n",
    "        region_name = AWS_REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dim and fact tables as binary format\n",
    "s3_resource.Object(bucket, 'output/factCovid.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dimRegion.to_csv(csv_buffer)\n",
    "s3_resource.Object(bucket, 'output/dimRegion.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dimDate.to_csv(csv_buffer)\n",
    "s3_resource.Object(bucket, 'output/dimDate.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dimHospital.to_csv(csv_buffer)\n",
    "s3_resource.Object(bucket, 'output/dimHospital.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting schema of dim and fact tables to be executed into redshift to create tables\n",
    "dimHospitalsql = pd.io.sql.get_schema(dimHospital.reset_index(), 'dimHospital')\n",
    "print(''.join(dimHospitalsql))\n",
    "dimDatesql = pd.io.sql.get_schema(dimDate.reset_index(), 'dimDate')\n",
    "print(''.join(dimDatesql))\n",
    "dimRegionsql = pd.io.sql.get_schema(dimRegion.reset_index(), 'dimRegion')\n",
    "print(''.join(dimRegionsql))\n",
    "factCovidsql = pd.io.sql.get_schema(factCovid.reset_index(), 'factCovid')\n",
    "print(''.join(factCovidsql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Redshift resource to deploy Dim and Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "\n",
    "conn = redshift_connector.connect(\n",
    "    host='HOST-REDSHIFT',\n",
    "    port=5439,\n",
    "    database='dev',\n",
    "    user='awsuser',\n",
    "    password='PASSWORD'\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = redshift_connector.Cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dim and Fact tables in redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "CREATE OR ALTER TABLE \"dimHospital\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" INTEGER,\n",
    "  \"state_name\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longtitude\" REAL,\n",
    "  \"hq_address\" TEXT,\n",
    "  \"hospital_name\" TEXT,\n",
    "  \"hospital_type\" TEXT,\n",
    "  \"hq_city\" TEXT,\n",
    "  \"hq_state\" TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE \"dimDate\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"date\" TIMESTAMP,\n",
    "  \"year\" INTEGER,\n",
    "  \"month\" INTEGER,\n",
    "  \"day\" INTEGER,\n",
    "  \"day_of_week\" INTEGER\n",
    ")\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE \"dimRegion\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longitude\" REAL,\n",
    "  \"county\" TEXT,\n",
    "  \"state\" TEXT\n",
    ")\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"CREATE TABLE \"factCovid\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"confirmed\" REAL,\n",
    "  \"deaths\" REAL,\n",
    "  \"recovered\" REAL,\n",
    "  \"active\" REAL,\n",
    "  \"date\" INTEGER,\n",
    "  \"positive\" INTEGER,\n",
    "  \"negative\" REAL,\n",
    "  \"hospitalizedcurrently\" REAL,\n",
    "  \"hospitalized\" REAL,\n",
    "  \"hospitalizeddischarged\" REAL\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy from S3 to RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this we need to create an IAM role from redshift to S3, giving permissions to copy data from S3\n",
    "#After, we need associated this role to the cluster in redshift\n",
    "cursor.execute(\"\"\"copy dimDate from 'STORAGE'\n",
    "               credentials 'IAM'\n",
    "               delimiter ','\n",
    "               region 'us-east-2'\n",
    "               IGNOREHEADER 1;\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
